{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/Q4RDa3YzhDq5GLjyTslE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JordanByrd2380/text-to-image-GAN/blob/main/cGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fQB46Ybtqzw",
        "outputId": "ab6aee22-318e-4b99-d22c-7e81b553b491"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQdl6NWvq4g",
        "outputId": "c987b574-f1c3-4d70-961b-6cc065f0b3b6"
      },
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cGAN.ipynb\t\t idxtoembed.pkl\n",
            " cs231n\t\t\t idxtotext.pkl\n",
            "'Data Collection.gdoc'\t'Introduction and Literature Review.gdoc'\n",
            " embeddings_index.pkl\t isbntocsv.pkl\n",
            "'Final Book Bundles'\t isbntoimg.pkl\n",
            "'Getting started.pdf'\t'Jordan RP Rehearsal Feedback Form.gslides'\n",
            " glove.6B.100d.txt\t __pycache__\n",
            " glove.6B.50d.txt\t'Results Conclusion.gdoc'\n",
            " helper.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txd_JUB_9pzn",
        "outputId": "691445ad-c8f5-4091-c889-8550c943a77f"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "rootdir = \"/content/drive/My Drive\"\n",
        "rootdir = os.path.join(rootdir, \"Final Book Bundles\")\n",
        "transform = transforms.Compose([#transforms.Resize(65),\n",
        "                                            #transforms.CenterCrop(64),\n",
        "                                            transforms.Resize(257),\n",
        "                                            transforms.CenterCrop(256),\n",
        "                                           transforms.ToTensor()])\n",
        "dataset = datasets.ImageFolder(rootdir, transform=transform)\n",
        "#dataset = datasets.ImageFolder(rootdir)\n",
        "ds = {}\n",
        "csvs = {}\n",
        "\n",
        "print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meMGV6mDiWEu",
        "outputId": "36b2edc5-af25-4079-a578-fe919977ee2d"
      },
      "source": [
        "import torch\n",
        "index = 0\n",
        "for pic in dataset:\n",
        "  if torch.count_nonzero(pic[0]- 1) > 49152:\n",
        "    ds[index] = pic[0]\n",
        "    index += 1\n",
        "print(len(ds))\n",
        "#counter = 0\n",
        "#ds2 = {}\n",
        "#counter = 0\n",
        "#for key in ds:\n",
        "#  for elem in ds[key]:\n",
        "#    if torch.count_nonzero(elem[0]- 1) > 3072:\n",
        "#      ds2[counter] = elem[0]\n",
        "#      counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecgNMC35qZzs"
      },
      "source": [
        "import pickle\n",
        "with open('ds.pkl', 'wb') as fid:\n",
        "     pickle.dump(ds, fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwgmVDtYAZpE"
      },
      "source": [
        "import os\n",
        "helper_loc = \"/content/drive/My Drive\"\n",
        "import sys\n",
        "sys.path.append(os.path.abspath(helper_loc))\n",
        "import helper\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "3GRE7FzVL2Vj",
        "outputId": "6be75946-38f2-487f-8e0d-77bf06df9ff7"
      },
      "source": [
        "#counter = 0\n",
        "#for i in range(4639):\n",
        "#  if torch.count_nonzero(dataset[i][0] - 1) != 0:\n",
        "#    counter += 1\n",
        "#  if counter % 100 == 0:\n",
        "#    print(counter)\n",
        "#print(counter)\n",
        "\n",
        "print(dataset.type)\n",
        "\n",
        "#dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=50)\n",
        "#torch.save(dataset, 'dataset2.pt')\n",
        "\n",
        "#images, labels = next(iter(dataloader))\n",
        "#helper.imshow(images[-1], normalize=False)\n",
        "#print(images[-1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1fb063487888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#print(counter)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGIVDe9ATkRe",
        "outputId": "3fba40eb-e52f-4953-c2df-c350871b4b89"
      },
      "source": [
        "idxtoimage= torch.zeros((4639, 3, 64, 64))\n",
        "for i in range(4639):\n",
        "  if i % 100 == 0:\n",
        "    print(i)\n",
        "  idxtoimage[i] = dataset[i][0]\n",
        "\n",
        "torch.save(idxtoimage, 'tensor.pt')\n",
        "#idxtoimage2[:] = dataset[:][0]\n",
        "#print(dataset[:, 0].shape)\n",
        "#print(len(dataset))\n",
        "#print(dataset[0])\n",
        "#images, idx = dataset\n",
        "#print(images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "3000\n",
            "3100\n",
            "3200\n",
            "3300\n",
            "3400\n",
            "3500\n",
            "3600\n",
            "3700\n",
            "3800\n",
            "3900\n",
            "4000\n",
            "4100\n",
            "4200\n",
            "4300\n",
            "4400\n",
            "4500\n",
            "4600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnD3LOlbgt7a",
        "outputId": "66b447e8-5d63-41ca-f2e4-73ffb1ae4fa6"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, tensor):\n",
        "    self.tensor = tensor\n",
        "  def __len__(self):\n",
        "    return self.tensor.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    return self.tensor[idx]\n",
        "\n",
        "tens = torch.ones((100, 3, 64, 64))\n",
        "dataset = ImageDataset(tens)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=50)\n",
        "for sample in dataloader:\n",
        "    right_images = sample\n",
        "    print(right_images.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 3, 64, 64])\n",
            "torch.Size([50, 3, 64, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqIE3Xyav1P3"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rootdir = \"/content/drive/My Drive\"\n",
        "rootdir = os.path.join(rootdir, \"Final Book Bundles\")\n",
        "\n",
        "\n",
        "csvs = {}\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "    if subdir.startswith(\"/content/drive/My Drive/Final Book Bundles/\") and not subdir.endswith(\"Im\") and not subdir.endswith(\"a\"):\n",
        "      for file in files:\n",
        "        if file.endswith(\".csv\") and not file.endswith(\"MD.csv\") and not file.startswith(\"MD\"):\n",
        "          booklines = pd.read_csv(os.path.join(subdir, file))\n",
        "          csvs[file[:-4]] = booklines\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfqE98OLPBR3"
      },
      "source": [
        " import pickle\n",
        "#with open('isbntocsv.pkl', 'wb') as fid:\n",
        "#     pickle.dump(csvs, fid)\n",
        "with open('drive/MyDrive/isbntoimg.pkl', 'rb') as fid:\n",
        "     ds = pickle.load(fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsrDd47GV93"
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(65),\n",
        "                                  transforms.CenterCrop(64),\n",
        "                                  transforms.ToTensor()])\n",
        "ds = {}\n",
        "imglist = []\n",
        "counter = 0\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "  if subdir.endswith(\"Im\"):\n",
        "    imagelist = imglist.copy()\n",
        "    for file in files:\n",
        "      period = file.index('.')\n",
        "      dash = file.index('-')\n",
        "      if file.endswith(\".jpg\"):\n",
        "        image = Image.open(os.path.join(subdir, file))\n",
        "        x = transform(image)\n",
        "        x = x.unsqueeze(0)\n",
        "        imagelist.append((x, file[dash+1:period]))\n",
        "\n",
        "    if subdir[-16:-3].startswith(\"780688012670\"):\n",
        "      ds[\"9780688012670\"] = imagelist\n",
        "    else:\n",
        "      ds[subdir[-16:-3]] = imagelist\n",
        "    counter += 1\n",
        "    print(\"Iteration: {}\".format(counter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIpHJipYNjJ-"
      },
      "source": [
        "#import pickle\n",
        "import torch\n",
        "import re\n",
        "#with open('drive/MyDrive/isbntoimg.pkl', 'rb') as fid:\n",
        "#     ds = pickle.load(fid)\n",
        "#with open('drive/MyDrive/idxtotext.pkl', 'rb') as fid:\n",
        "#     idxtotext = pickle.load(fid)   \n",
        "#helper.imshow(ds['9780688012670'][-1][0], normalize=False)\n",
        "def embed_dict(idxtotext):\n",
        "  for key in idxtotext.keys():\n",
        "    embed = torch.zeros(0)\n",
        "    \n",
        "    text, isbn, idx = idxtotext[key]\n",
        "\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = text.strip('\\n')\n",
        "    text = text.strip('\\r')\n",
        "    text = re.split('[^a-zA-Z]', text)\n",
        "\n",
        "    count = 0\n",
        "    for wordidx in range(len(text)):\n",
        "      if count == 10:\n",
        "        break\n",
        "      if text[wordidx] in embeddings_index.keys():\n",
        "        embed = torch.cat([embed, torch.from_numpy(embeddings_index[text[wordidx]])], 0)\n",
        "        count += 1\n",
        "    if len(embed) != 1000:\n",
        "      remaining = 1000 - len(embed)\n",
        "      embed = torch.cat([embed, torch.zeros(remaining)], 0)\n",
        "    assert len(embed) == 1000\n",
        "    idxtotext[key] = embed, isbn, idx\n",
        "\n",
        "embed_dict(idxtotext)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqRrXWPCdEK"
      },
      "source": [
        "with open('drive/MyDrive/embeddings_index.pkl', 'rb') as fid:\n",
        "     embeddings_index = pickle.load(fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "sTMsHazGPd1o",
        "outputId": "34c7b215-c831-4054-e336-527e51951b5f"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "\n",
        "#csvs[\"9780688012670\"] = csvs[\"9780688012670 \"]\n",
        "#csvs.pop(\"9780688012670 \", None)\n",
        "class ImageDatasetConditional(Dataset):\n",
        "  def __init__(self, idxtotext, isbntoimg):\n",
        "    self.idxtotext = idxtotext\n",
        "    self.isbntoimg = isbntoimg\n",
        "  def __len__(self):\n",
        "    return len(idxtotext)\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()    \n",
        "    embed, isbn, item = self.idxtotext[idx]\n",
        "    image = torch.ones((3, 64, 64))\n",
        "    for elem in self.isbntoimg[isbn]:\n",
        "      if elem[1] == str(item):\n",
        "        image = elem[0]\n",
        "    return {'image': image, 'text': embed}\n",
        "dataset = ImageDatasetConditional(idxtotext, ds)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-39f49e6ec561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#csvs[\"9780688012670\"] = csvs[\"9780688012670 \"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#csvs.pop(\"9780688012670 \", None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: module functions cannot set METH_CLASS or METH_STATIC"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAjj8lMZ0ScG"
      },
      "source": [
        "\n",
        "#!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43wjBC6T8Q1X"
      },
      "source": [
        "#VANILLA GAN\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class generator(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(generator, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.noise_dim = 100\n",
        "\t\tself.ngf = 64\n",
        "\n",
        "\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "\t\tself.netG = nn.Sequential(\n",
        "\t\t\tnn.ConvTranspose2d(self.noise_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 8),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*8) x 4 x 4\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 4),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*4) x 8 x 8\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 2),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*2) x 16 x 16\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf) x 32 x 32\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.Tanh()\n",
        "\t\t\t # state size. (num_channels) x 64 x 64\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, z):\n",
        "\t\treturn self.netG(z)\n",
        "\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(discriminator, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.ndf = 64\n",
        "\n",
        "\t\tself.B_dim = 128\n",
        "\t\tself.C_dim = 16\n",
        "\n",
        "\t\tself.netD_1 = nn.Sequential(\n",
        "\t\t\t# input is (nc) x 64 x 64\n",
        "\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf) x 32 x 32\n",
        "\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 2),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*2) x 16 x 16\n",
        "\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 4),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*4) x 8 x 8\n",
        "\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 8),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True)\n",
        "\t\t)\n",
        "\n",
        "\t\tself.netD_2 = nn.Sequential(\n",
        "\t\t\t# state size. (ndf*8) x 4 x 4\n",
        "\t\t\tnn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.Sigmoid()\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, inp):\n",
        "\t\tx_intermediate = self.netD_1(inp)\n",
        "\t\toutput =  self.netD_2(x_intermediate)\n",
        "\t\treturn output.view(-1, 1).squeeze(1), x_intermediate\n",
        "\n",
        "\n",
        "model = generator()\n",
        "model.load_state_dict(torch.load('gengan_249.pth', map_location=torch.device('cpu')))\n",
        "#x = torch.rand(1,100, 1, 1)\n",
        "#out = model.forward(x)\n",
        "#out = torch.from_numpy(out.detach().numpy())\n",
        "#print(out)\n",
        "#helper.imshow(out.squeeze(0), normalize=False)\n",
        "\n",
        "\n",
        "#with open('ds2.pkl', 'rb') as fid:\n",
        "#    ds2 = pickle.load(fid)\n",
        "\n",
        "#min = 10000\n",
        "#for key in ds2.keys():\n",
        "  #sumval = torch.sum(abs(ds2[key] - out))\n",
        "  #sumval = torch.linalg.norm(ds2[key] - out)\n",
        "  #print(sumval)\n",
        "  #if sumval < min:\n",
        "    #copy = ds2[key]\n",
        "    #min = sumval\n",
        "  #print(ds2[key] - out)\n",
        "#print(torch.std(copy- out))\n",
        "#helper.imshow(copy, normalize=False)\n",
        "#print(min)\n",
        "\n",
        "imagez = torch.zeros((256, 3, 64, 64))\n",
        "for i in range(256):\n",
        "  x = torch.rand(1,100, 1, 1)\n",
        "  out = model.forward(x)\n",
        "  out = torch.from_numpy(out.detach().numpy())\n",
        "  imagez[i] = out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlD9svPKN1QD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 50\n",
        "#generator = generator()\n",
        "#discriminator = discriminator()\n",
        "discriminator.to(device)\n",
        "generator.to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "optimD = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
        "optimG = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
        "\n",
        "\n",
        "l1_coef = 0\n",
        "l2_coef = 0\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "l2_loss = nn.MSELoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "iteration = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for sample in dataloader:\n",
        "        iteration += 1\n",
        "        right_images = sample[0]\n",
        "\n",
        "        right_images = Variable(right_images.float()).cuda()\n",
        "\n",
        "        real_labels = torch.ones(right_images.size(0))\n",
        "        fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "        # ======== One sided label smoothing ==========\n",
        "        # Helps preventing the discriminator from overpowering the\n",
        "        # generator adding penalty when the discriminator is too confident\n",
        "        # =============================================\n",
        "        #smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "        smoothed_real_labels = torch.FloatTensor(real_labels.numpy() - 0.1)\n",
        "        real_labels = Variable(real_labels).cuda()\n",
        "        smoothed_real_labels = Variable(smoothed_real_labels).cuda()\n",
        "        fake_labels = Variable(fake_labels).cuda()\n",
        "\n",
        "\n",
        "        # Train the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        outputs, activation_real = discriminator(right_images)\n",
        "        real_loss = criterion(outputs, smoothed_real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "        noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "        noise = noise.view(right_images.size(0), 100, 1, 1)\n",
        "        fake_images = generator(noise)\n",
        "        outputs, _ = discriminator(fake_images)\n",
        "        fake_loss = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "\n",
        "        d_loss = real_loss + fake_loss\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimD.step()\n",
        "\n",
        "        # Train the generator\n",
        "        generator.zero_grad()\n",
        "        noise = Variable(torch.randn(right_images.size(0), 100)).cuda()\n",
        "        noise = noise.view(right_images.size(0), 100, 1, 1)\n",
        "        fake_images = generator(noise)\n",
        "        outputs, activation_fake = discriminator(fake_images)\n",
        "        _, activation_real = discriminator(right_images)\n",
        "\n",
        "        activation_fake = torch.mean(activation_fake, 0)\n",
        "        activation_real = torch.mean(activation_real, 0)\n",
        "\n",
        "        # ======= Generator Loss function============\n",
        "        # This is a customized loss function, the first term is the regular cross entropy loss\n",
        "        # The second term is feature matching loss, this measure the distance between the real and generated\n",
        "        # images statistics by comparing intermediate layers activations\n",
        "        # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
        "        # because it links the embedding feature vector directly to certain pixel values.\n",
        "        g_loss = criterion(outputs, real_labels) \\\n",
        "                  + l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n",
        "                  + l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimG.step()\n",
        "\n",
        "        if iteration % 5 == 0:\n",
        "            #print(\"5 iterations done\")\n",
        "            print(\"D_loss = {}\".format(d_loss))\n",
        "            print(\"G loss = {}\".format(g_loss))\n",
        "            #self.logger.log_iteration_gan(epoch, d_loss, g_loss, real_score, fake_score)\n",
        "            #self.logger.draw(right_images, fake_images)\n",
        "\n",
        "    #self.logger.plot_epoch_w_scores(iteration)\n",
        "\n",
        "    #if (epoch) % 50 == 0:\n",
        "        #Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, epoch)\n",
        "    if epoch % 50 == 0:\n",
        "      #SAVE\n",
        "      #torch.save(model.state_dict(), PATH)\n",
        "\n",
        "      #LOAD\n",
        "      #model = TheModelClass(*args, **kwargs)\n",
        "      #model.load_state_dict(torch.load(PATH))\n",
        "      #model.eval()\n",
        "\n",
        "      path = os.path.join(dir_path, subdir_path)\n",
        "      torch.save(netD.state_dict(), '{0}/disc_{1}.pth'.format(path, epoch))\n",
        "      torch.save(netG.state_dict(), '{0}/gen_{1}.pth'.format(path, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LbkSV40UQcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ea4a67-8d02-4576-ddbe-c3c7515ada74"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-31 17:57:31--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-05-31 17:57:31--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-05-31 17:57:31--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 40s  \n",
            "\n",
            "2021-05-31 18:00:12 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QumGDJOA6Nn",
        "outputId": "8031b635-ad96-4ddc-a34f-f8d09d36229c"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krmyMSyaBWCW",
        "outputId": "147af267-9bcc-4724-aa99-b0d705651bfb"
      },
      "source": [
        "import numpy as np\n",
        "print('Indexing word vectors.')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open('drive/MyDrive/glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Indexing word vectors.\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCphLUfU0Rx3"
      },
      "source": [
        "with open('embeddings_index.pkl', 'wb') as fid:\n",
        "  pickle.dump(embeddings_index, fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "pBtSWeQKtag4",
        "outputId": "7cb5505f-7f9f-4625-d132-4bd30d8d6e0c"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pdb\n",
        "import pickle\n",
        "\n",
        "\n",
        "class Concat_embed(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dim, projected_embed_dim):\n",
        "        super(Concat_embed, self).__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(in_features=embed_dim, out_features=projected_embed_dim),\n",
        "            nn.BatchNorm1d(num_features=projected_embed_dim),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, inp, embed):\n",
        "        projected_embed = self.projection(embed)\n",
        "        replicated_embed = projected_embed.repeat(4, 4, 1, 1).permute(2,  3, 0, 1)\n",
        "        hidden_concat = torch.cat([inp, replicated_embed], 1)\n",
        "\n",
        "        return hidden_concat\n",
        "\n",
        "\n",
        "\n",
        "class condgenerator(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(condgenerator, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.noise_dim = 100\n",
        "\t\tself.embed_dim = 1000\n",
        "\t\tself.projected_embed_dim = 128\n",
        "\t\tself.latent_dim = self.noise_dim + self.projected_embed_dim\n",
        "\t\tself.ngf = 64\n",
        "\n",
        "\t\tself.projection = nn.Sequential(\n",
        "\t\t\tnn.Linear(in_features=self.embed_dim, out_features=self.projected_embed_dim),\n",
        "\t\t\tnn.BatchNorm1d(num_features=self.projected_embed_dim),\n",
        "\t\t\tnn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\t\t\t)\n",
        "\n",
        "\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "\t\tself.netG = nn.Sequential(\n",
        "\t\t\tnn.ConvTranspose2d(self.latent_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 8),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*8) x 4 x 4\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 4),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*4) x 8 x 8\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 2),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*2) x 16 x 16\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf) x 32 x 32\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.Tanh()\n",
        "\t\t\t # state size. (num_channels) x 64 x 64\n",
        "\t\t\t)\n",
        "\n",
        "\n",
        "\tdef forward(self, embed_vector, z):\n",
        "\n",
        "\t\tprojected_embed = self.projection(embed_vector).unsqueeze(2).unsqueeze(3)\n",
        "\t\tlatent_vector = torch.cat([projected_embed, z], 1)\n",
        "\t\toutput = self.netG(latent_vector)\n",
        "\n",
        "\t\treturn output\n",
        "\n",
        "class conddiscriminator(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(conddiscriminator, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.embed_dim = 1000\n",
        "\t\tself.projected_embed_dim = 128\n",
        "\t\tself.ndf = 64\n",
        "\t\tself.B_dim = 128\n",
        "\t\tself.C_dim = 16\n",
        "\n",
        "\t\tself.netD_1 = nn.Sequential(\n",
        "\t\t\t# input is (nc) x 64 x 64\n",
        "\t\t\tnn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf) x 32 x 32\n",
        "\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 2),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*2) x 16 x 16\n",
        "\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 4),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*4) x 8 x 8\n",
        "\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 8),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t)\n",
        "\n",
        "\t\tself.projector = Concat_embed(self.embed_dim, self.projected_embed_dim)\n",
        "\n",
        "\t\tself.netD_2 = nn.Sequential(\n",
        "\t\t\t# state size. (ndf*8) x 4 x 4\n",
        "\t\t\tnn.Conv2d(self.ndf * 8 + self.projected_embed_dim, 1, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.Sigmoid()\n",
        "\t\t\t)\t\n",
        "\n",
        "\tdef forward(self, inp, embed):\n",
        "\t\tx_intermediate = self.netD_1(inp)\n",
        "\t\tx = self.projector(x_intermediate, embed)\n",
        "\t\tx = self.netD_2(x)\n",
        "\n",
        "\t\treturn x.view(-1, 1).squeeze(1) , x_intermediate\n",
        "\n",
        "\n",
        "\n",
        "model = condgenerator()\n",
        "model.load_state_dict(torch.load('gen_190.pth', map_location=torch.device('cpu')))\n",
        "x = torch.rand(10, 100, 1, 1)\n",
        "#y = torch.zeros(10,1000)\n",
        "with open(\"drive/MyDrive/embeddings_index.pkl\", \"rb\") as fp:\n",
        "  embeddings_index= pickle.load(fp)\n",
        "vec = embeddings_index[\"the\"]\n",
        "vec2 = embeddings_index[\"clear\"]\n",
        "vec4 = embeddings_index[\"blue\"]\n",
        "vec5 = embeddings_index[\"sky\"]\n",
        "\n",
        "y = torch.cat([torch.tensor(vec), torch.tensor(vec2), torch.tensor(vec3), torch.tensor(vec4),torch.zeros(600)], 0)\n",
        "y = y.unsqueeze(0)\n",
        "print(y.shape)\n",
        "tens = torch.zeros((10, 1000))\n",
        "tens += y\n",
        "#tens = torch.rand(10, 1000)\n",
        "#tens[0] = y\n",
        "out = model(tens, x)\n",
        "out = torch.from_numpy(out.detach().numpy())\n",
        "#print(out)\n",
        "print(out.shape)\n",
        "helper.imshow(out[0], normalize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3bf4d6ba13b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mvec5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sky\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vec3' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "lPoCgsantzmw",
        "outputId": "2e06a382-5b5c-4c56-92b2-e5b7335f88fd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 50\n",
        "generator = condgenerator()\n",
        "discriminator = conddiscriminator()\n",
        "discriminator.to(device)\n",
        "generator.to(device)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "optimD = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
        "optimG = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
        "\n",
        "\n",
        "l1_coef = 50\n",
        "l2_coef = 100\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "l2_loss = nn.MSELoss()\n",
        "l1_loss = nn.L1Loss()\n",
        "iteration = 0\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    for sample in dataloader:\n",
        "        iteration += 1\n",
        "        right_images = sample['image']\n",
        "        right_embed = sample['text']\n",
        "\n",
        "\n",
        "        right_images = Variable(right_images.float())\n",
        "        right_embed = Variable(right_embed.float())\n",
        "\n",
        "        real_labels = torch.ones(right_images.size(0))\n",
        "        fake_labels = torch.zeros(right_images.size(0))\n",
        "\n",
        "        # ======== One sided label smoothing ==========\n",
        "        # Helps preventing the discriminator from overpowering the\n",
        "        # generator adding penalty when the discriminator is too confident\n",
        "        # =============================================\n",
        "        #smoothed_real_labels = torch.FloatTensor(Utils.smooth_label(real_labels.numpy(), -0.1))\n",
        "        smoothed_real_labels = torch.FloatTensor(real_labels.numpy() - 0.1)\n",
        "\n",
        "        real_labels = Variable(real_labels)\n",
        "        smoothed_real_labels = Variable(smoothed_real_labels)\n",
        "        fake_labels = Variable(fake_labels)\n",
        "\n",
        "        # Train the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        outputs, activation_real = discriminator(right_images, right_embed)\n",
        "        real_loss = criterion(outputs, smoothed_real_labels)\n",
        "        real_score = outputs\n",
        "\n",
        "\n",
        "        noise = Variable(torch.randn(right_images.size(0), 100))\n",
        "        noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "        fake_images = generator(right_embed, noise)\n",
        "        outputs, _ = discriminator(fake_images, right_embed)\n",
        "        fake_loss = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "\n",
        "        d_loss = real_loss + fake_loss\n",
        "\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimD.step()\n",
        "\n",
        "        # Train the generator\n",
        "        generator.zero_grad()\n",
        "        noise = Variable(torch.randn(right_images.size(0), 100))\n",
        "        noise = noise.view(noise.size(0), 100, 1, 1)\n",
        "        fake_images = generator(right_embed, noise)\n",
        "        outputs, activation_fake = discriminator(fake_images, right_embed)\n",
        "        _, activation_real = discriminator(right_images, right_embed)\n",
        "\n",
        "        activation_fake = torch.mean(activation_fake, 0)\n",
        "        activation_real = torch.mean(activation_real, 0)\n",
        "\n",
        "\n",
        "        #======= Generator Loss function============\n",
        "        # This is a customized loss function, the first term is the regular cross entropy loss\n",
        "        # The second term is feature matching loss, this measure the distance between the real and generated\n",
        "        # images statistics by comparing intermediate layers activations\n",
        "        # The third term is L1 distance between the generated and real images, this is helpful for the conditional case\n",
        "        # because it links the embedding feature vector directly to certain pixel values.\n",
        "        #===========================================\n",
        "        g_loss = criterion(outputs, real_labels) \\\n",
        "                  + l2_coef * l2_loss(activation_fake, activation_real.detach()) \\\n",
        "                  + l1_coef * l1_loss(fake_images, right_images)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimG.step()\n",
        "\n",
        "        if iteration % 5 == 0:\n",
        "          g_losses.append(g_loss)\n",
        "          d_losses.append(d_loss)\n",
        "        #    self.logger.log_iteration_gan(epoch,d_loss, g_loss, real_score, fake_score)\n",
        "        #    self.logger.draw(right_images, fake_images)\n",
        "\n",
        "    #self.logger.plot_epoch_w_scores(epoch)\n",
        "\n",
        "        if (iteration) % 5 == 0:\n",
        "          with open(\"d_losses{}.txt\".format(epoch), \"wb\") as fp:\n",
        "            pickle.dump(d_losses, fp)\n",
        "          with open(\"g_losses{}.txt\".format(epoch), \"wb\") as fp:\n",
        "            pickle.dump(g_losses, fp)\n",
        "        #Utils.save_checkpoint(self.discriminator, self.generator, self.checkpoints_path, self.save_path, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-3bbe64978c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m   1510\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX0KZV_WVUfF",
        "outputId": "bbc3bc89-3230-4c89-8254-4cb33bb43e2d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from tqdm import trange\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,      # First max pooling features\n",
        "        192: 1,     # Second max pooling featurs\n",
        "        768: 2,     # Pre-aux classifier features\n",
        "        2048: 3,    # Final average pooling features\n",
        "        'prob': 4,  # softmax layer\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False,\n",
        "                 use_fid_inception=True):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        # assert self.last_needed_block <= 3, \\\n",
        "        #     'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        #if use_fid_inception:\n",
        "        #    inception = fid_inception_v3()\n",
        "        #else:\n",
        "        #    inception = models.inception_v3(\n",
        "         #       pretrained=True, init_weights=False)\n",
        "\n",
        "        inception = models.inception_v3(pretrained=True, init_weights=False)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            self.fc = inception.fc\n",
        "            self.fc.bias = None\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        if self.last_needed_block >= 4:\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            # N x 2048 x 1 x 1\n",
        "            x = torch.flatten(x, 1)\n",
        "            # N x 2048\n",
        "            x = self.fc(x)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            outp.append(x)\n",
        "\n",
        "        return outp\n",
        "\n",
        "\n",
        "\n",
        "def get_inception_score(images, splits=10, batch_size=32, use_torch=False,\n",
        "                        verbose=False, parallel=False):\n",
        "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM['prob']\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    if parallel:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    preds = []\n",
        "    iterator = trange(\n",
        "        0, len(images), batch_size, dynamic_ncols=True, leave=False,\n",
        "        disable=not verbose, desc=\"get_inception_score\")\n",
        "\n",
        "    for start in iterator:\n",
        "        end = start + batch_size\n",
        "        batch_images = images[start: end]\n",
        "        batch_images = torch.from_numpy(batch_images).type(torch.FloatTensor)\n",
        "        batch_images = batch_images.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch_images)[0]\n",
        "        if use_torch:\n",
        "            preds.append(pred)\n",
        "        else:\n",
        "            preds.append(pred.cpu().numpy())\n",
        "    if use_torch:\n",
        "        preds = torch.cat(preds, 0)\n",
        "    else:\n",
        "        preds = np.concatenate(preds, 0)\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        part = preds[\n",
        "            (i * preds.shape[0] // splits):\n",
        "            ((i + 1) * preds.shape[0] // splits), :]\n",
        "        if use_torch:\n",
        "            kl = part * (\n",
        "                torch.log(part) -\n",
        "                torch.log(torch.unsqueeze(torch.mean(part, 0), 0)))\n",
        "            kl = torch.mean(torch.sum(kl, 1))\n",
        "            scores.append(torch.exp(kl))\n",
        "        else:\n",
        "            kl = part * (\n",
        "                np.log(part) -\n",
        "                np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
        "            kl = np.mean(np.sum(kl, 1))\n",
        "            scores.append(np.exp(kl))\n",
        "    if use_torch:\n",
        "        scores = torch.stack(scores)\n",
        "        is_mean, is_std = (\n",
        "            torch.mean(scores).cpu().item(), torch.std(scores).cpu().item())\n",
        "    else:\n",
        "        is_mean, is_std = np.mean(scores), np.std(scores)\n",
        "    del preds, scores, model\n",
        "    return is_mean, is_std\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#var = torch.rand(10, 3, 64, 64)\n",
        "#var[:7] = imagez[:] \n",
        "#print(imagez[0].shape)\n",
        "\n",
        "#mean, std = get_inception_score(np.array())\n",
        "#print(imagez.shape)\n",
        "\n",
        "mean, std = get_inception_score((np.array(imagez)))\n",
        "print(mean)\n",
        "print(std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4694201\n",
            "0.06556692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UCafSpPh8RR"
      },
      "source": [
        "import torch\n",
        "counter = 0\n",
        "ds2 = {}\n",
        "counter = 0\n",
        "for key in ds:\n",
        "  for elem in ds[key]:\n",
        "    if torch.count_nonzero(elem[0]- 1) > 3072:\n",
        "      ds2[counter] = elem[0]\n",
        "      counter += 1\n",
        " # ds2[counter] = include"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYv0N3eWjBT5",
        "outputId": "39734fea-8551-478c-de60-69961fc21b35"
      },
      "source": [
        "with open('ds2.pkl', 'wb') as fid:\n",
        "     pickle.dump(ds2, fid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.9294, 0.6863, 0.4275,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         [0.9333, 0.6118, 0.2118,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         [0.9333, 0.7294, 0.5569,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         ...,\n",
            "         [0.8824, 0.8863, 0.8627,  ..., 0.8039, 0.9529, 0.9961],\n",
            "         [0.8824, 0.8824, 0.8627,  ..., 0.8000, 0.9686, 0.9961],\n",
            "         [0.8745, 0.8745, 0.8627,  ..., 0.8510, 0.9647, 0.9961]],\n",
            "\n",
            "        [[0.9529, 0.7882, 0.5451,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         [0.9569, 0.7529, 0.4667,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         [0.9647, 0.8000, 0.5373,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         ...,\n",
            "         [0.8863, 0.8863, 0.8667,  ..., 0.8000, 0.9569, 1.0000],\n",
            "         [0.8824, 0.8824, 0.8275,  ..., 0.7961, 0.9725, 1.0000],\n",
            "         [0.8784, 0.8784, 0.8627,  ..., 0.7647, 0.9529, 1.0000]],\n",
            "\n",
            "        [[0.8941, 0.6745, 0.4118,  ..., 1.0000, 0.9922, 0.9922],\n",
            "         [0.8902, 0.6392, 0.1686,  ..., 1.0000, 1.0000, 0.9961],\n",
            "         [0.8980, 0.7373, 0.2824,  ..., 1.0000, 1.0000, 1.0000],\n",
            "         ...,\n",
            "         [0.8431, 0.8392, 0.8196,  ..., 0.7647, 0.9059, 0.9412],\n",
            "         [0.8431, 0.8392, 0.7529,  ..., 0.7647, 0.9176, 0.9373],\n",
            "         [0.8431, 0.8353, 0.8039,  ..., 0.7373, 0.8941, 0.9294]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI1uNAPIkkxB",
        "outputId": "b5d3e94f-cf99-46ad-ce10-91d84fc1600d"
      },
      "source": [
        "import numpy\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy import asarray\n",
        "from numpy.random import randint\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets.mnist import load_data\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf \n",
        "# scale an array of images to a new size\n",
        "def scale_images(images, new_shape):\n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)\n",
        " \n",
        "# calculate frechet inception distance\n",
        "def calculate_fid(model, images1, images2):\n",
        "\t# calculate activations\n",
        "\tact1 = model.predict(images1)\n",
        "\tact2 = model.predict(images2)\n",
        "\t# calculate mean and covariance statistics\n",
        "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
        "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
        "\t# calculate sum squared difference between means\n",
        "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
        "\t# calculate sqrt of product between cov\n",
        "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
        "\t# check and correct imaginary numbers from sqrt\n",
        "\tif iscomplexobj(covmean):\n",
        "\t\tcovmean = covmean.real\n",
        "\t# calculate score\n",
        "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\treturn fid\n",
        " \n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
        "# define two fake collections of images\n",
        "images1 = randint(0, 255, 256*32*32*3)\n",
        "images1 = images1.reshape((256,32,32,3))\n",
        "images2 = randint(0, 255, 256*32*32*3)\n",
        "images2 = images2.reshape((256,32,32,3))\n",
        "print('Prepared', images1.shape, images2.shape)\n",
        "# convert integer to floating point values\n",
        "images1 = images1.astype('float32')\n",
        "images2 = images2.astype('float32')\n",
        "# resize images\n",
        "images1 = scale_images(images1, (299,299,3))\n",
        "images2 = scale_images(images2, (299,299,3))\n",
        "print('Scaled', images1.shape, images2.shape)\n",
        "# pre-process images\n",
        "images1 = preprocess_input(images1)\n",
        "images2 = preprocess_input(images2)\n",
        "# fid between images1 and images1\n",
        "fid = calculate_fid(model, images1, images1)\n",
        "print('FID (same): %.3f' % fid)\n",
        "# fid between images1 and images2\n",
        "fid = calculate_fid(model, images1, images2)\n",
        "print('FID (different): %.3f' % fid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prepared (256, 32, 32, 3) (256, 32, 32, 3)\n",
            "Scaled (256, 299, 299, 3) (256, 299, 299, 3)\n",
            "FID (same): -0.000\n",
            "FID (different): 7.363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "kn8VdPzslOWI",
        "outputId": "b3f27839-fd9c-44bc-a002-d59e6305edaf"
      },
      "source": [
        "\n",
        "#with open('ds2.pkl', 'rb') as fid:\n",
        "#    ds2 = pickle.load(fid)\n",
        "#images1 = ds2[:2]\n",
        "images1 = randint((2, 3, 64, 64))\n",
        "images1[0] = ds2[0]\n",
        "images1[1] = ds2[1]\n",
        "#images1 = randint((2,32,32,3))\n",
        "\n",
        "#images1 = images1.reshape((1, 32, 32, 3))\n",
        "#images1 = images1.unsqueeze(0)\n",
        "images1 = images1.permute(0, 3, 1, 2 )\n",
        "images2 = imagez[:2]\n",
        "#images2 = images2.reshape((1, 32, 32, 3))\n",
        "#images2 = images2.unsqueeze(0)\n",
        "images2 = images2.permute(0, 3, 1, 2 )\n",
        "\n",
        "images1 = scale_images(images1, (299, 299, 3))\n",
        "images2 = scale_images(images2, (299, 299, 3))\n",
        "images1 = tf.convert_to_tensor(preprocess_input(images1))\n",
        "images2 = tf.convert_to_tensor(preprocess_input(images2))\n",
        "print(images1.shape)\n",
        "print(images2.shape)\n",
        "fid = calculate_fid(model, images1, images2)\n",
        "print('FID (different): %.3f' % fid)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-a7e3db92165f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#images1 = ds2[:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimages1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimages1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimages1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#images1 = randint((2,32,32,3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aE_W0MLdZxr"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from torch import optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "\n",
        "class generator2(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(generator2, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.noise_dim = 100\n",
        "\t\tself.ngf = 64\n",
        "\n",
        "\t\t# based on: https://github.com/pytorch/examples/blob/master/dcgan/main.py\n",
        "\t\tself.netG = nn.Sequential(\n",
        "\n",
        "\n",
        "\t\t\tnn.ConvTranspose2d(self.noise_dim, self.ngf * 8, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 8),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*8) x 4 x 4\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 4),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*4) x 8 x 8\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf * 2),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf*2) x 16 x 16\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf * 2,self.ngf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ngf),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\t# state size. (ngf) x 32 x 32\n",
        "            nn.ConvTranspose2d(self.ngf, self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "            # (ngf x 64 x 64)\n",
        "            nn.ConvTranspose2d(self.ngf , self.ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.ngf),\n",
        "            nn.ReLU(True),\n",
        "            # (ngf x 128 x 128)\n",
        "\t\t\tnn.ConvTranspose2d(self.ngf, self.num_channels, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.Tanh()\n",
        "\t\t\t # state size. (num_channels) x 256 x 256\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, z):\n",
        "\t\treturn self.netG(z)\n",
        "\n",
        "\n",
        "class discriminator2(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(discriminator2, self).__init__()\n",
        "\t\tself.image_size = 64\n",
        "\t\tself.num_channels = 3\n",
        "\t\tself.ndf = 64\n",
        "\n",
        "\t\tself.B_dim = 128\n",
        "\t\tself.C_dim = 16\n",
        "\n",
        "\t\tself.netD_1 = nn.Sequential(\n",
        "            #input is (nc) x 256 x 256\n",
        "            nn.Conv2d(self.num_channels, self.ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            #input is (ndf) x 128 x 128\n",
        "            nn.Conv2d(self.ndf, self.ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# input is (ndf) x 64 x 64\n",
        "\t\t\tnn.Conv2d(self.ndf, self.ndf, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf) x 32 x 32\n",
        "\t\t\tnn.Conv2d(self.ndf, self.ndf * 2, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 2),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*2) x 16 x 16\n",
        "\t\t\tnn.Conv2d(self.ndf * 2, self.ndf * 4, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 4),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "\t\t\t# state size. (ndf*4) x 8 x 8\n",
        "\t\t\tnn.Conv2d(self.ndf * 4, self.ndf * 8, 4, 2, 1, bias=False),\n",
        "\t\t\tnn.BatchNorm2d(self.ndf * 8),\n",
        "\t\t\tnn.LeakyReLU(0.2, inplace=True)\n",
        "\t\t)\n",
        "\n",
        "\t\tself.netD_2 = nn.Sequential(\n",
        "\t\t\t# state size. (ndf*8) x 4 x 4\n",
        "\t\t\tnn.Conv2d(self.ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "\t\t\tnn.Sigmoid()\n",
        "\t\t\t)\n",
        "\n",
        "\tdef forward(self, inp):\n",
        "\t\tx_intermediate = self.netD_1(inp)\n",
        "\t\toutput =  self.netD_2(x_intermediate)\n",
        "\t\treturn output.view(-1, 1).squeeze(1), x_intermediate\n",
        "\n",
        "\n",
        "\n",
        "model = generator2()\n",
        "model.load_state_dict(torch.load('gengannew_49.pth', map_location=torch.device('cpu')))\n",
        "x = torch.rand(1,100, 1, 1)\n",
        "out = model.forward(x)\n",
        "out = torch.from_numpy(out.detach().numpy())\n",
        "#print(out)\n",
        "helper.imshow(out.squeeze(0), normalize=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}